#!/bin/bash
# Autogenerated by ClearML SLURM integration.

#
# Slurm job name.
#
#SBATCH --job-name=train_uck_ctr
#######################

#
# Slurm out log file.
#
#SBATCH --output="output-%A_%a.out"
#######################

#
# Slurm err log file.
#
#SBATCH --error="error-%A_%a.err"
#######################

#
# Job walltime.
#
#SBATCH --time=00-08:00:00
#######################

#
# GPUs per node.
#
#SBATCH --gres=gpu:1
#######################

#
# CPUs per node.
#
#SBATCH --cpus-per-task=8
#######################

#
# Requested nodes.
#
#SBATCH --mem=40960
#######################

#
# Signal that will be send before killing a job.
#
#SBATCH --signal=USR1@60
#######################

#
#
# Cluster specific options.
#
#

#
# 
#
#SBATCH --partition=plgrid-gpu-v100
#######################

#
# 
#
#SBATCH --constraint=localfs
#######################

cd $SLURM_SUBMIT_DIR
export PYTHONPATH=$SLURM_SUBMIT_DIR

srun /net/people/plgszymswiat/.conda/envs/ctr/bin/python train_scripts/train_uck_ctr.py +experiment=$1
